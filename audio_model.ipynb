{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6a0uFknQHyA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel, CLIPProcessor, CLIPModel\n",
        "from transformers import WhisperProcessor, WhisperModel\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Mount Google Drive ------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aRYoYGMgQI1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Set Checkpoint Directory ------------------\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/MultimodalModelCheckpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "jb-H6af0QKSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Set Seed ------------------\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "dT5w8WVLQMxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Preprocessing Helpers ------------------\n",
        "def extract_audio_embedding(audio_path, device='cpu', duration=30):\n",
        "    # Load model and processor\n",
        "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "    model = WhisperModel.from_pretrained(\"openai/whisper-base\").to(device)\n",
        "\n",
        "    # Load the audio file\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "    # Ensure mono audio (single channel)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert stereo to mono by averaging channels\n",
        "\n",
        "    # Trim to the first 30 seconds if necessary\n",
        "    num_samples = int(duration * sr)\n",
        "    waveform = waveform[:, :num_samples]\n",
        "\n",
        "    # Resample to 16 kHz if necessary\n",
        "    if sr != 16000:\n",
        "        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=16000)\n",
        "\n",
        "    # Process the waveform, ensuring the batch size is 1\n",
        "    inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The input features shape may need to be manually adjusted.\n",
        "    # Whisper expects shape [batch_size, num_channels, sequence_length] where `num_channels` is the feature dimension.\n",
        "    input_features = inputs['input_features']\n",
        "\n",
        "    # Ensure it's a 3D tensor in the form [batch_size, num_channels, sequence_length]\n",
        "    if input_features.ndimension() == 2:\n",
        "        input_features = input_features.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "    # Use only the encoder to obtain embeddings\n",
        "    encoder = model.get_encoder()\n",
        "\n",
        "    # Run the encoder and get the embeddings\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs = encoder(input_features=input_features.to(device))\n",
        "\n",
        "    # Average over all time steps to get the entire context\n",
        "    final_hidden_state = encoder_outputs.last_hidden_state.mean(dim=1)  # [1, 512] - average over all time steps\n",
        "\n",
        "    return final_hidden_state  # This should return a tensor of shape [1, 512]\n",
        "\n",
        "# def extract_frame(video_path, timestamp_str):\n",
        "#     print(video_path)\n",
        "#     time_in_seconds = float(timestamp_str)\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "#     if not cap.isOpened() or fps == 0:\n",
        "#         raise ValueError(\"Error opening video file or unable to retrieve FPS.\")\n",
        "\n",
        "#     frame_number = int(fps * time_in_seconds)\n",
        "#     cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
        "#     success, frame = cap.read()\n",
        "#     cap.release()\n",
        "\n",
        "#     if not success or frame is None:\n",
        "#         raise ValueError(f\"Could not extract frame at {timestamp_str} seconds\")\n",
        "\n",
        "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "#     return Image.fromarray(frame)\n",
        "\n",
        "# def get_image_embedding(image, clip_processor, clip_model, device='cpu'):\n",
        "#     inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "#     with torch.no_grad():\n",
        "#         image_embed = clip_model.get_image_features(**inputs)\n",
        "#     return image_embed.squeeze(0)  # [H_img]"
      ],
      "metadata": {
        "id": "Dobr2i2nQOQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import WhisperProcessor, WhisperModel\n",
        "\n",
        "class TokenAudioDataset(Dataset):\n",
        "    def __init__(self, texts, audio_paths, labels, tokenizer, max_len, device):\n",
        "        self.texts = texts\n",
        "        self.audio_paths = audio_paths\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "\n",
        "        # Whisper model and processor (shared for efficiency)\n",
        "        self.whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "        self.whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\").to(device).eval()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        audio_path = self.audio_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize text\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt',\n",
        "            is_split_into_words=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids = encoded['input_ids'].squeeze(0)\n",
        "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Audio Embedding\n",
        "        audio_embedding = self.extract_audio_embedding(audio_path)\n",
        "        audio_embedding = audio_embedding.unsqueeze(0).repeat(self.max_len, 1)  # [T, H_a]\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'audio_embedding': audio_embedding,\n",
        "            'labels': torch.tensor(label[:self.max_len], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def extract_audio_embedding(self, audio_path, duration=30):\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Trim and resample\n",
        "        num_samples = int(duration * sr)\n",
        "        waveform = waveform[:, :num_samples]\n",
        "\n",
        "        if sr != 16000:\n",
        "            waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=16000)\n",
        "\n",
        "        inputs = self.whisper_processor(\n",
        "            waveform.squeeze().numpy(),\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        input_features = inputs['input_features']\n",
        "\n",
        "        # Pad or truncate to consistent length\n",
        "        if input_features.shape[-1] < 3000:\n",
        "            pad_len = 3000 - input_features.shape[-1]\n",
        "            input_features = torch.nn.functional.pad(input_features, (0, pad_len), mode='constant', value=0)\n",
        "        elif input_features.shape[-1] > 3000:\n",
        "            input_features = input_features[:, :, :3000]\n",
        "\n",
        "        if input_features.ndimension() == 2:\n",
        "            input_features = input_features.unsqueeze(0)\n",
        "\n",
        "        encoder = self.whisper_model.get_encoder()\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs = encoder(input_features=input_features.to(self.device))\n",
        "\n",
        "        return encoder_outputs.last_hidden_state.mean(dim=1).squeeze(0)  # [512]\n"
      ],
      "metadata": {
        "id": "064Gom0R4rMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import Dataset\n",
        "# import torch\n",
        "# import torchaudio\n",
        "# from transformers import WhisperProcessor, WhisperModel\n",
        "# import os\n",
        "\n",
        "# class TokenAudioVisualDataset(Dataset):\n",
        "#     def __init__(self, texts, timestamps, audio_paths, video_paths, labels, tokenizer, clip_processor, clip_model, max_len, device):\n",
        "#         self.texts = texts\n",
        "#         self.timestamps = timestamps\n",
        "#         self.audio_paths = audio_paths\n",
        "#         self.video_paths = video_paths\n",
        "#         self.labels = labels\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_len = max_len\n",
        "#         self.clip_processor = clip_processor\n",
        "#         self.clip_model = clip_model\n",
        "#         self.device = device\n",
        "\n",
        "#         # Whisper model and processor (shared for efficiency)\n",
        "#         self.whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "#         self.whisper_model = WhisperModel.from_pretrained(\"openai/whisper-base\").to(device).eval()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text = self.texts[idx]\n",
        "#         timestamps = self.timestamps[idx]\n",
        "#         audio_path = self.audio_paths[idx]\n",
        "#         video_path = self.video_paths[idx]\n",
        "#         label = self.labels[idx]\n",
        "\n",
        "#         # Tokenize text\n",
        "#         encoded = self.tokenizer(\n",
        "#             text,\n",
        "#             truncation=True,\n",
        "#             padding='max_length',\n",
        "#             max_length=self.max_len,\n",
        "#             return_tensors='pt',\n",
        "#             is_split_into_words=True,\n",
        "#             return_attention_mask=True\n",
        "#         )\n",
        "#         input_ids = encoded['input_ids'].squeeze(0)\n",
        "#         attention_mask = encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "#         # Audio Embedding (lazy loading)\n",
        "#         audio_embedding = self.extract_audio_embedding(audio_path)\n",
        "#         audio_embedding = audio_embedding.unsqueeze(0).repeat(self.max_len, 1)  # [T, H_a]\n",
        "\n",
        "#         # Video Embedding per timestamp\n",
        "#         image_embeds = []\n",
        "#         for ts in timestamps[:self.max_len]:\n",
        "#             frame = extract_frame(video_path, ts)\n",
        "#             img_embed = get_image_embedding(frame, self.clip_processor, self.clip_model, device=self.device)\n",
        "#             image_embeds.append(img_embed)\n",
        "\n",
        "#         # Pad if fewer frames than max_len\n",
        "#         while len(image_embeds) < self.max_len:\n",
        "#             image_embeds.append(torch.zeros_like(image_embeds[0]))\n",
        "#         image_embeds = torch.stack(image_embeds, dim=0)  # [T, H_img]\n",
        "\n",
        "#         return {\n",
        "#             'input_ids': input_ids,\n",
        "#             'attention_mask': attention_mask,\n",
        "#             'audio_embedding': audio_embedding,\n",
        "#             'image_embedding': image_embeds,\n",
        "#             'labels': torch.tensor(label[:self.max_len], dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "#     def extract_audio_embedding(self, audio_path, duration=30):\n",
        "#         waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "#         if waveform.shape[0] > 1:\n",
        "#             waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "#         # Trim and resample\n",
        "#         num_samples = int(duration * sr)\n",
        "#         waveform = waveform[:, :num_samples]\n",
        "\n",
        "#         if sr != 16000:\n",
        "#             waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=16000)\n",
        "\n",
        "#         inputs = self.whisper_processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "#         input_features = inputs['input_features']\n",
        "\n",
        "#         input_features = inputs['input_features']\n",
        "#         if input_features.shape[-1] < 3000:\n",
        "#             pad_len = 3000 - input_features.shape[-1]\n",
        "#             input_features = torch.nn.functional.pad(input_features, (0, pad_len), mode='constant', value=0)\n",
        "#         elif input_features.shape[-1] > 3000:\n",
        "#             input_features = input_features[:, :, :3000]  # truncate\n",
        "\n",
        "#         if input_features.ndimension() == 2:\n",
        "#             input_features = input_features.unsqueeze(0)\n",
        "\n",
        "#         encoder = self.whisper_model.get_encoder()\n",
        "#         with torch.no_grad():\n",
        "#             encoder_outputs = encoder(input_features=input_features.to(self.device))\n",
        "#         return encoder_outputs.last_hidden_state.mean(dim=1).squeeze(0)  # [512]"
      ],
      "metadata": {
        "id": "nfmRoyq7qsTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ------------------ Dataset ------------------\n",
        "# class TokenAudioVisualDataset(Dataset):\n",
        "#     def __init__(self, texts, timestamps, audio_path, video_path, labels, tokenizer, clip_processor, clip_model, max_len, device):\n",
        "#         self.texts = texts\n",
        "#         self.timestamps = timestamps\n",
        "#         self.labels = labels\n",
        "#         self.video_path = video_path\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_len = max_len\n",
        "#         self.clip_processor = clip_processor\n",
        "#         self.clip_model = clip_model\n",
        "#         self.device = device\n",
        "\n",
        "#         audio_embedding_seq = extract_audio_embedding(audio_path, device=device)\n",
        "#         self.audio_embedding = audio_embedding_seq.mean(dim=0)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text = self.texts[idx]\n",
        "#         timestamps = self.timestamps[idx]\n",
        "#         label = self.labels[idx]\n",
        "\n",
        "#         encoded = self.tokenizer(\n",
        "#             text,\n",
        "#             truncation=True,\n",
        "#             padding='max_length',\n",
        "#             max_length=self.max_len,\n",
        "#             return_tensors='pt',\n",
        "#             is_split_into_words=True,\n",
        "#             return_attention_mask=True\n",
        "#         )\n",
        "\n",
        "#         input_ids = encoded['input_ids'].squeeze(0)\n",
        "#         attention_mask = encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "#         audio_embed = self.audio_embedding\n",
        "#         if audio_embed.dim() == 2:\n",
        "#             audio_embed = audio_embed.mean(dim=0)\n",
        "#         audio_embed = audio_embed.unsqueeze(0).repeat(self.max_len, 1)  # [T, H_a]\n",
        "\n",
        "#         image_embeds = []\n",
        "#         for ts in timestamps[:self.max_len]:\n",
        "#             frame = extract_frame(self.video_path, ts)\n",
        "#             img_embed = get_image_embedding(frame, self.clip_processor, self.clip_model, device=self.device)\n",
        "#             image_embeds.append(img_embed)\n",
        "\n",
        "#         while len(image_embeds) < self.max_len:\n",
        "#             image_embeds.append(torch.zeros_like(image_embeds[0]))\n",
        "\n",
        "#         image_embeds = torch.stack(image_embeds, dim=0)  # [T, H_img]\n",
        "\n",
        "#         return {\n",
        "#             'input_ids': input_ids,\n",
        "#             'attention_mask': attention_mask,\n",
        "#             'audio_embedding': audio_embed,\n",
        "#             'image_embedding': image_embeds,\n",
        "#             'labels': torch.tensor(label[:self.max_len], dtype=torch.long)\n",
        "#         }"
      ],
      "metadata": {
        "id": "CySQAut7QRy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Model ------------------\n",
        "# class RobertaAudioVisualClassifier(nn.Module):\n",
        "#     def __init__(self, audio_dim=512, image_dim=512, hidden_size=256, num_labels=3):\n",
        "#         super().__init__()\n",
        "#         self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(self.roberta.config.hidden_size + audio_dim + image_dim, hidden_size),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.1),\n",
        "#             nn.Linear(hidden_size, num_labels)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask, audio_embedding, image_embedding):\n",
        "#         outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         roberta_out = outputs.last_hidden_state  # [B, T, H_text]\n",
        "#         fused = torch.cat([roberta_out, audio_embedding, image_embedding], dim=-1)\n",
        "#         logits = self.classifier(fused)\n",
        "#         return logits\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "\n",
        "# ------------------ Model ------------------\n",
        "class RobertaAudioClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim=512, hidden_size=256, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.roberta.config.hidden_size + audio_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_embedding):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        roberta_out = outputs.last_hidden_state  # [B, T, H_text]\n",
        "        fused = torch.cat([roberta_out, audio_embedding], dim=-1)  # [B, T, H_text + H_audio]\n",
        "        logits = self.classifier(fused)  # [B, T, num_labels]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "J7EwI4JjQUdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# import torch\n",
        "# import os\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# # ------------------ Train & Eval ------------------\n",
        "# def train(model, dataloader, optimizer, loss_fn, device, epoch=None, save_path=None):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\" if epoch is not None else \"Training\")\n",
        "\n",
        "#     for batch in progress_bar:\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         audio_embedding = batch['audio_embedding'].to(device)\n",
        "#         image_embedding = batch['image_embedding'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         logits = model(input_ids, attention_mask, audio_embedding, image_embedding)\n",
        "\n",
        "#         labels = labels[:len(logits)]\n",
        "#         padding_len = logits.size(1) - labels.size(1)\n",
        "#         if padding_len > 0:\n",
        "#             pad = torch.full((1, padding_len), -100, dtype=labels.dtype, device=labels.device)\n",
        "#             labels = torch.cat([labels, pad], dim=1)\n",
        "#         else:\n",
        "#             labels = labels[:, :logits.size(1)]\n",
        "\n",
        "#         loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#         progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "#     avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "#     if save_path and epoch is not None:\n",
        "#         ckpt_path = os.path.join(save_path, f\"model_epoch_{epoch}.pt\")\n",
        "#         torch.save(model.state_dict(), ckpt_path)\n",
        "#         print(f\"✅ Saved model at: {ckpt_path}\")\n",
        "\n",
        "#     return avg_loss\n",
        "\n",
        "# def evaluate(model, dataloader, device):\n",
        "#     model.eval()\n",
        "#     all_preds, all_labels = [], []\n",
        "#     progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in progress_bar:\n",
        "#             input_ids = batch['input_ids'].to(device)\n",
        "#             attention_mask = batch['attention_mask'].to(device)\n",
        "#             audio_embedding = batch['audio_embedding'].to(device)\n",
        "#             image_embedding = batch['image_embedding'].to(device)\n",
        "#             labels = batch['labels'].to(device)\n",
        "\n",
        "#             logits = model(input_ids, attention_mask, audio_embedding, image_embedding)\n",
        "\n",
        "#             labels = labels[:len(logits)]\n",
        "#             padding_len = logits.size(1) - labels.size(1)\n",
        "#             if padding_len > 0:\n",
        "#                 pad = torch.full((1, padding_len), -100, dtype=labels.dtype, device=labels.device)\n",
        "#                 labels = torch.cat([labels, pad], dim=1)\n",
        "#             else:\n",
        "#                 labels = labels[:, :logits.size(1)]\n",
        "\n",
        "#             preds = torch.argmax(logits, dim=-1)\n",
        "#             mask = attention_mask.view(-1) == 1\n",
        "#             all_preds.extend(preds.view(-1)[mask].cpu().numpy())\n",
        "#             all_labels.extend(labels.view(-1)[mask].cpu().numpy())\n",
        "\n",
        "#     filtered_preds = []\n",
        "#     filtered_labels = []\n",
        "#     for p, l in zip(all_preds, all_labels):\n",
        "#         if l != -100:\n",
        "#             filtered_preds.append(p)\n",
        "#             filtered_labels.append(l)\n",
        "\n",
        "#     print(classification_report(filtered_labels, filtered_preds, target_names=[\"None\", \"Emotion\", \"Cause\"], digits=4))\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ------------------ Train & Eval ------------------\n",
        "def train(model, dataloader, optimizer, loss_fn, device, epoch=None, save_path=None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\" if epoch is not None else \"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        audio_embedding = batch['audio_embedding'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask, audio_embedding)\n",
        "\n",
        "        labels = labels[:len(logits)]\n",
        "        padding_len = logits.size(1) - labels.size(1)\n",
        "        if padding_len > 0:\n",
        "            pad = torch.full((labels.size(0), padding_len), -100, dtype=labels.dtype, device=labels.device)\n",
        "            labels = torch.cat([labels, pad], dim=1)\n",
        "        else:\n",
        "            labels = labels[:, :logits.size(1)]\n",
        "\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    if save_path and epoch is not None:\n",
        "        ckpt_path = os.path.join(save_path, f\"model_epoch_{epoch}.pt\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        print(f\"✅ Saved model at: {ckpt_path}\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    total_loss = 0.0  # To track the total loss\n",
        "    progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            audio_embedding = batch['audio_embedding'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask, audio_embedding)\n",
        "\n",
        "            labels = labels[:, :logits.size(1)]  # Truncate labels to match the logits size\n",
        "            padding_len = logits.size(1) - labels.size(1)\n",
        "            if padding_len > 0:\n",
        "                pad = torch.full((labels.size(0), padding_len), -100, dtype=labels.dtype, device=labels.device)\n",
        "                labels = torch.cat([labels, pad], dim=1)\n",
        "            else:\n",
        "                labels = labels[:, :logits.size(1)]\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            mask = attention_mask.view(-1) == 1\n",
        "            all_preds.extend(preds.view(-1)[mask].cpu().numpy())\n",
        "            all_labels.extend(labels.view(-1)[mask].cpu().numpy())\n",
        "\n",
        "    # Calculate the average validation loss\n",
        "    avg_val_loss = total_loss / len(dataloader)\n",
        "\n",
        "    # Filtering predictions and labels by removing padding (-100)\n",
        "    filtered_preds = []\n",
        "    filtered_labels = []\n",
        "    for p, l in zip(all_preds, all_labels):\n",
        "        if l != -100:\n",
        "            filtered_preds.append(p)\n",
        "            filtered_labels.append(l)\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(filtered_labels, filtered_preds, target_names=[\"None\", \"Emotion\", \"Cause\"], digits=4))\n",
        "\n",
        "    return avg_val_loss  # Return the average validation loss\n"
      ],
      "metadata": {
        "id": "B1RhecSiQXOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label_map = {'None': 0, 'Emotion': 1, 'Cause': 2}\n",
        "# def parse_tokens(token_data):\n",
        "#     texts = [item['token'] for item in token_data]\n",
        "#     timestamps = [str(item['start']) for item in token_data]\n",
        "#     labels = [label_map.get(item['label'] or 'None', 0) for item in token_data]\n",
        "#     return texts, timestamps, labels\n",
        "\n",
        "# from transformers import RobertaTokenizer\n",
        "# import re\n",
        "# import json\n",
        "\n",
        "# # Load RoBERTa tokenizer\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# # Step 1: Parse the .txt file\n",
        "# def parse_transcript_file(file_path):\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         lines = f.readlines()\n",
        "\n",
        "#     full_text = \"\"\n",
        "#     segments = []\n",
        "\n",
        "#     # Extract full transcript text\n",
        "#     text_started = False\n",
        "#     for line in lines:\n",
        "#         if line.strip().startswith(\"Transcript:\"):\n",
        "#             text_started = True\n",
        "#             continue\n",
        "#         if line.strip().startswith(\"Labeled Segments:\"):\n",
        "#             break\n",
        "#         if text_started:\n",
        "#             full_text += line.strip() + \" \"\n",
        "\n",
        "#     # Extract labeled segments\n",
        "#     segment_lines = lines[lines.index(\"Labeled Segments:\\n\") + 1:]\n",
        "#     for line in segment_lines:\n",
        "#         match = re.match(r'Time:\\s*([\\d.]+)-[\\d.]+\\s*sec\\s*\\|\\s*Label:\\s*(\\w+|None)\\s*\\|\\s*Text:\\s*\"(.*?)\"', line)\n",
        "#         if match:\n",
        "#             start = float(match.group(1))\n",
        "#             label = match.group(2) if match.group(2) != \"None\" else None\n",
        "#             word = match.group(3)\n",
        "#             segments.append((word, start, label))\n",
        "\n",
        "#     return full_text.strip(), segments\n",
        "\n",
        "# # Step 2: Tokenize and align\n",
        "# def tokenize_and_align(text, segments):\n",
        "#     tokens = []\n",
        "#     seg_index = 0\n",
        "\n",
        "#     for word, start, label in segments:\n",
        "#         # Tokenize the word using RoBERTa\n",
        "#         word_tokens = tokenizer.tokenize(word)\n",
        "#         for tok in word_tokens:\n",
        "#             tokens.append({\n",
        "#                 'token': tok,\n",
        "#                 'start': start,\n",
        "#                 'label': label\n",
        "#             })\n",
        "\n",
        "#     return tokens\n",
        "\n",
        "label_map = {'None': 0, 'Emotion': 1, 'Cause': 2}\n",
        "\n",
        "def parse_tokens(token_data):\n",
        "    texts = [item['token'] for item in token_data]\n",
        "    timestamps = [str(item['start']) for item in token_data]\n",
        "    labels = [label_map.get(item['label'] or 'None', 0) for item in token_data]\n",
        "    return texts, timestamps, labels\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "import re\n",
        "\n",
        "# Load RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Step 1: Parse the .txt file\n",
        "def parse_transcript_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    full_text = \"\"\n",
        "    segments = []\n",
        "\n",
        "    # Extract full transcript text\n",
        "    text_started = False\n",
        "    for line in lines:\n",
        "        if line.strip().startswith(\"Transcript:\"):\n",
        "            text_started = True\n",
        "            continue\n",
        "        if line.strip().startswith(\"Labeled Segments:\"):\n",
        "            break\n",
        "        if text_started:\n",
        "            full_text += line.strip() + \" \"\n",
        "\n",
        "    # Extract labeled segments\n",
        "    segment_lines = lines[lines.index(\"Labeled Segments:\\n\") + 1:]\n",
        "    for line in segment_lines:\n",
        "        match = re.match(r'Time:\\s*([\\d.]+)-[\\d.]+\\s*sec\\s*\\|\\s*Label:\\s*(\\w+|None)\\s*\\|\\s*Text:\\s*\"(.*?)\"', line)\n",
        "        if match:\n",
        "            start = float(match.group(1))\n",
        "            label = match.group(2) if match.group(2) != \"None\" else None\n",
        "            word = match.group(3)\n",
        "            segments.append((word, start, label))\n",
        "\n",
        "    return full_text.strip(), segments\n",
        "\n",
        "# Step 2: Tokenize and align\n",
        "def tokenize_and_align(text, segments):\n",
        "    tokens = []\n",
        "    for word, start, label in segments:\n",
        "        word_tokens = tokenizer.tokenize(word)\n",
        "        for tok in word_tokens:\n",
        "            tokens.append({\n",
        "                'token': tok,\n",
        "                'start': start,\n",
        "                'label': label\n",
        "            })\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "WNgmyzl_Qf7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_mask = [item['attention_mask'] for item in batch]\n",
        "    audio_embedding = [item['audio_embedding'] for item in batch]\n",
        "    labels = [item['labels'] for item in batch]\n",
        "\n",
        "    # Pad sequences to ensure all tensors have the same length\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    audio_embedding = pad_sequence(audio_embedding, batch_first=True, padding_value=0)\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'audio_embedding': audio_embedding,\n",
        "        'labels': labels\n",
        "    }"
      ],
      "metadata": {
        "id": "BsLFuWfD7hA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, random\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "# clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# video_folder = \"/content/drive/MyDrive/nlpProject/Videos\"\n",
        "# audio_folder = \"/content/drive/MyDrive/nlpProject/Audios\"\n",
        "# txt_folder = \"/content/drive/MyDrive/nlpProject/Done_text\"\n",
        "\n",
        "# triplets = []\n",
        "# for fname in os.listdir(video_folder):\n",
        "#     base = os.path.splitext(fname)[0]\n",
        "#     v_path = os.path.join(video_folder, fname)\n",
        "#     a_path = os.path.join(audio_folder, base + \".mp3\")\n",
        "#     t_path = os.path.join(txt_folder, base + \".txt\")\n",
        "#     if os.path.exists(a_path) and os.path.exists(t_path):\n",
        "#         triplets.append((t_path, a_path, v_path))\n",
        "\n",
        "# data = []\n",
        "# for txt_path, audio_path, video_path in triplets:\n",
        "#     text, segments = parse_transcript_file(txt_path)\n",
        "#     token_data = tokenize_and_align(text, segments)\n",
        "#     texts, timestamps, labels = parse_tokens(token_data)\n",
        "#     data.append((texts, timestamps, labels, audio_path, video_path))\n",
        "\n",
        "# train_data, val_test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "# val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# def unpack_split(split_data):\n",
        "#     texts, timestamps, labels, audio_paths, video_paths = zip(*split_data)\n",
        "#     return list(texts), list(timestamps), list(labels), list(audio_paths), list(video_paths)\n",
        "\n",
        "# train_texts, train_timestamps, train_labels, train_audios, train_videos = unpack_split(train_data)\n",
        "# val_texts, val_timestamps, val_labels, val_audios, val_videos = unpack_split(val_data)\n",
        "# test_texts, test_timestamps, test_labels, test_audios, test_videos = unpack_split(test_data)\n",
        "\n",
        "# # train_dataset = TokenAudioVisualDataset(train_texts, train_timestamps, train_labels, train_audios, train_videos, tokenizer, clip_processor, clip_model, max_len=16, device=device)\n",
        "# # val_dataset = TokenAudioVisualDataset(val_texts, val_timestamps, val_labels, val_audios, val_videos, tokenizer, clip_processor, clip_model, max_len=16, device=device)\n",
        "# # test_dataset = TokenAudioVisualDataset(test_texts, test_timestamps, test_labels, test_audios, test_videos, tokenizer, clip_processor, clip_model, max_len=16, device=device)\n",
        "\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # Create the train, val, and test datasets\n",
        "# train_dataset = TokenAudioVisualDataset(\n",
        "#     train_texts, train_timestamps, train_audios, train_videos, train_labels,\n",
        "#     tokenizer, clip_processor, clip_model, max_len=512, device=device\n",
        "# )\n",
        "\n",
        "# val_dataset = TokenAudioVisualDataset(\n",
        "#     val_texts, val_timestamps, val_audios, val_videos, val_labels,\n",
        "#     tokenizer, clip_processor, clip_model, max_len=512, device=device\n",
        "# )\n",
        "\n",
        "# test_dataset = TokenAudioVisualDataset(\n",
        "#     test_texts, test_timestamps, test_audios, test_videos, test_labels,\n",
        "#     tokenizer, clip_processor, clip_model, max_len=512, device=device\n",
        "# )\n",
        "\n",
        "# # Wrap datasets in dataloaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "import os, random\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Data directories\n",
        "audio_folder = \"/content/drive/MyDrive/nlpProject/Audios\"\n",
        "txt_folder = \"/content/drive/MyDrive/nlpProject/Done_text\"\n",
        "\n",
        "# Build triplets: only text + audio\n",
        "triplets = []\n",
        "for fname in os.listdir(txt_folder):\n",
        "    base = os.path.splitext(fname)[0]\n",
        "    t_path = os.path.join(txt_folder, fname)\n",
        "    a_path = os.path.join(audio_folder, base + \".mp3\")\n",
        "    if os.path.exists(a_path):\n",
        "        triplets.append((t_path, a_path))\n",
        "\n",
        "# Process data\n",
        "data = []\n",
        "for txt_path, audio_path in triplets:\n",
        "    text, segments = parse_transcript_file(txt_path)\n",
        "    token_data = tokenize_and_align(text, segments)\n",
        "    texts, timestamps, labels = parse_tokens(token_data)\n",
        "    data.append((texts, timestamps, labels, audio_path))\n",
        "\n",
        "# Train/val/test split\n",
        "train_data, val_test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Unpack utility\n",
        "def unpack_split(split_data):\n",
        "    texts, timestamps, labels, audio_paths = zip(*split_data)\n",
        "    return list(texts), list(timestamps), list(labels), list(audio_paths)\n",
        "\n",
        "# Unpack each split\n",
        "train_texts, train_timestamps, train_labels, train_audios = unpack_split(train_data)\n",
        "val_texts, val_timestamps, val_labels, val_audios = unpack_split(val_data)\n",
        "test_texts, test_timestamps, test_labels, test_audios = unpack_split(test_data)\n",
        "\n",
        "# Dataset class should now only handle text + audio\n",
        "train_dataset = TokenAudioDataset(\n",
        "    texts=train_texts,\n",
        "    audio_paths=train_audios,\n",
        "    labels=train_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=512,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# For validation dataset\n",
        "val_dataset = TokenAudioDataset(\n",
        "    texts=val_texts,\n",
        "    audio_paths=val_audios,\n",
        "    labels=val_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=512,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# For test dataset\n",
        "test_dataset = TokenAudioDataset(\n",
        "    texts=test_texts,\n",
        "    audio_paths=test_audios,\n",
        "    labels=test_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=512,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "b8TII9UyQiK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import glob\n",
        "# import re\n",
        "# import torch\n",
        "\n",
        "# model = RobertaAudioVisualClassifier().to(device)\n",
        "\n",
        "# # Find all checkpoint files like: model_epoch_*.pt\n",
        "# checkpoint_files = sorted(\n",
        "#     glob.glob(os.path.join(CHECKPOINT_DIR, \"model_epoch_*.pt\"))\n",
        "# )\n",
        "\n",
        "# latest_epoch = 0  # Default if no checkpoints found\n",
        "\n",
        "# if checkpoint_files:\n",
        "#     # Extract epoch numbers and find the latest one\n",
        "#     epochs = [\n",
        "#         int(re.search(r\"model_epoch_(\\d+)\\.pt\", f).group(1))\n",
        "#         for f in checkpoint_files\n",
        "#         if re.search(r\"model_epoch_(\\d+)\\.pt\", f)\n",
        "#     ]\n",
        "#     latest_epoch = max(epochs)\n",
        "#     latest_ckpt_path = os.path.join(CHECKPOINT_DIR, f\"model_epoch_{latest_epoch}.pt\")\n",
        "\n",
        "#     model.load_state_dict(torch.load(latest_ckpt_path, map_location=device))\n",
        "#     print(f\"✅ Loaded model from {latest_ckpt_path} (epoch {latest_epoch})\")\n",
        "# else:\n",
        "#     print(\"⚠️ No checkpoint found, training from scratch.\")\n",
        "\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "# loss_fn = nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor([0.05, 0.475, 0.475], device=device))\n",
        "\n",
        "# best_val_loss, patience, wait = float('inf'), 3, 0\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Initialize your text+audio model\n",
        "model = RobertaAudioClassifier().to(device)  # Replace with your new class name\n",
        "\n",
        "# Find all saved checkpoint files like: model_epoch_*.pt\n",
        "checkpoint_files = sorted(\n",
        "    glob.glob(os.path.join(CHECKPOINT_DIR, \"model_epoch_*.pt\"))\n",
        ")\n",
        "\n",
        "latest_epoch = 0  # Default if no checkpoint is found\n",
        "\n",
        "if checkpoint_files:\n",
        "    # Extract epoch numbers and find the latest\n",
        "    epochs = [\n",
        "        int(re.search(r\"model_epoch_(\\d+)\\.pt\", f).group(1))\n",
        "        for f in checkpoint_files\n",
        "        if re.search(r\"model_epoch_(\\d+)\\.pt\", f)\n",
        "    ]\n",
        "    latest_epoch = max(epochs)\n",
        "    latest_ckpt_path = os.path.join(CHECKPOINT_DIR, f\"model_epoch_{latest_epoch}.pt\")\n",
        "\n",
        "    model.load_state_dict(torch.load(latest_ckpt_path, map_location=device))\n",
        "    print(f\"✅ Loaded model from {latest_ckpt_path} (epoch {latest_epoch})\")\n",
        "else:\n",
        "    print(\"⚠️ No checkpoint found, training from scratch.\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Loss function with class weights (assumes same label mapping)\n",
        "class_weights = torch.tensor([0.05, 0.475, 0.475], device=device)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100, weight=class_weights)\n",
        "\n",
        "# Early stopping params\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "wait = 0"
      ],
      "metadata": {
        "id": "D2KHTYXsksaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn, device, epoch=epoch, save_path=CHECKPOINT_DIR)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "print(\"Final evaluation on test set:\")\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "J7NE0mfCNW62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}